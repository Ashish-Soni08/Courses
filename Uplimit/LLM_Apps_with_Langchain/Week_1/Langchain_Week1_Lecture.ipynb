{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidhusmart/CoRise_Prompt_Design_Course/blob/cohort3/Week_3/CoRise_Week3_LectureNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook is part of the course: [LLM Apps with Langchain](https://uplimit.com/course/llm-apps-with-langchain) and is created by Sidharth Ramachandran to be covered during the Lecture session of Week 1."
      ],
      "metadata": {
        "id": "rhAeTUqiM8Iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary Imports"
      ],
      "metadata": {
        "id": "b9ZZiPDOmo2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries to get it out of the way\n",
        "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai"
      ],
      "metadata": {
        "id": "vHDu2dKNNHSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c4407d7-5afa-451b-9773-3df9eb2d7b42"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.8/379.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.9/335.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=userdata.get('openai_api'))"
      ],
      "metadata": {
        "id": "VEoqh5S8NQB1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api')\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('langsmith_api')"
      ],
      "metadata": {
        "id": "KGoVvSenBOq_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the basics of Langchain"
      ],
      "metadata": {
        "id": "10E0If-imztZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a Prompt Template?\n",
        "\n",
        "The PromptTemplate is an abstraction class provided by Langchain to hold the chat messages that are sent to the LLM. Having this as a dedicated class allows us to save and version prompts and also standardize them to accept vertain parameters that can be passed at runtime. It's one way to parameterize prompts.\n",
        "\n",
        "Most LLMs have now standardized to the chat template with System and Human messages followed by the AI message that contains the response from the LLM API."
      ],
      "metadata": {
        "id": "ARrMXED6NYHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "from langchain.prompts import SystemMessagePromptTemplate\n",
        "from langchain.prompts import HumanMessagePromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"You are a helpful summarization assistant.\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"\"\"Identify the highlights from the following piece of text and return it as a bullet list\n",
        "                                                while focussing on retaining as much specific details as you can.\n",
        "                                                The returned bullet list must be formatted as \\n- Fact 1 about the information \\n- Fact 2\n",
        "                                                about the information.\n",
        "                                                {input_text}\n",
        "                                              \"\"\")\n",
        "])"
      ],
      "metadata": {
        "id": "GthzYh0qUTDb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to create an instance of the PromptTemplate you can provide it with any text value like so. You can observe the entire instance and the values it contains."
      ],
      "metadata": {
        "id": "Ziz_dGceVSn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Club-Mate (German pronunciation: [ˈklʊp ˈmaːtə]) is a caffeinated carbonated mate-extract beverage made by the Loscher Brewery (Brauerei Loscher)\n",
        "in Münchsteinach, Germany, which originated in 1924. Club-Mate has 20 mg of caffeine per 100 ml, sugar content of 5 g per 100 ml, and 20 kcal per 100 ml,\n",
        "which is lower than most energy drinks. Club-Mate is available in 0.33-litre and 0.5-litre bottles.\n",
        "\n",
        "Some Club-Mate bottles include the slogan \"man gewöhnt sich daran\", which roughly translates as \"you'll get used to it\". Examples of Club-Mate-based\n",
        "mixed drinks are: vodka-mate; Tschunk,[2][3] a combination of rum and Club-Mate; Jaeger-Mate, a mix of Jägermeister and Club-Mate....\"\n",
        "\"\"\"\n",
        "\n",
        "summarization_prompt = prompt_template.invoke({\"input_text\": text})"
      ],
      "metadata": {
        "id": "4KYpsb_LVcCI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSZBXakoVqVa",
        "outputId": "304f4a30-176d-4ec9-b119-606803d81cb4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful summarization assistant.'), HumanMessage(content='Identify the highlights from the following piece of text and return it as a bullet list\\n                                                while focussing on retaining as much specific details as you can.\\n                                                The returned bullet list must be formatted as \\n- Fact 1 about the information \\n- Fact 2\\n                                                about the information.\\n                                                \\nClub-Mate (German pronunciation: [ˈklʊp ˈmaːtə]) is a caffeinated carbonated mate-extract beverage made by the Loscher Brewery (Brauerei Loscher)\\nin Münchsteinach, Germany, which originated in 1924. Club-Mate has 20 mg of caffeine per 100 ml, sugar content of 5 g per 100 ml, and 20 kcal per 100 ml,\\nwhich is lower than most energy drinks. Club-Mate is available in 0.33-litre and 0.5-litre bottles.\\n\\nSome Club-Mate bottles include the slogan \"man gewöhnt sich daran\", which roughly translates as \"you\\'ll get used to it\". Examples of Club-Mate-based\\nmixed drinks are: vodka-mate; Tschunk,[2][3] a combination of rum and Club-Mate; Jaeger-Mate, a mix of Jägermeister and Club-Mate....\"\\n\\n                                              ')])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This abstraction allows you to standardize your prompts into a template, specify variables (you can include multiple) and pass them in at runtime. The response will be contained in another class called AIMessage which will be populated once you have connected this prompt with an LLM. We will see that subsequently."
      ],
      "metadata": {
        "id": "btdaY-ivNmZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sharing and Re-using prompt templates\n",
        "\n",
        "One of the biggest challenges when an organization starts developing LLM apps has been the management of prompts. This can be within a certain repository or across the organization or community. Langchain provides a Prompt Hub that serves as a public collection of prompts used by various members of the community. You can pull and reuse any of these prompts like so.\n"
      ],
      "metadata": {
        "id": "Ghvdi-DPanIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "# pull a chat prompt\n",
        "rag_prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "BxKU6rQvYMG8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75mIW9Z1ahgk",
        "outputId": "98c3a60a-61f8-4484-e3b1-7e5a85a304da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will notice in the above example that the user has not used an SystemMessage in the prompt but has provided all instructions into the HumanMessage directly. There is no one best way to do this and typically the use of the SystemMessage is to assign a persona or specific style to the model as discussed in the [OpenAI API documentation](https://platform.openai.com/docs/guides/chat-completions/message-roles).\n",
        "\n",
        "In the same way that you can pull prompts, you can also save your own prompts by pushing it to the Langchain hub. You need to provide it with a particular `handle` - a unique english language identifier for your prompt. In the following example, we can save the summarization prompt template that we created earlier to our private hub space with the handle - \"summarization-prompt\""
      ],
      "metadata": {
        "id": "nJIV_ADCbU5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "\n",
        "hub.push(\"summarization-prompt\", prompt_template, new_repo_is_public=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qgLz8IJLciwE",
        "outputId": "b3166ef8-53f3-4c59-f43f-70d15097b96e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://smith.langchain.com/prompts/summarization-prompt/af851947?organizationId=1fc832ac-2867-5171-a7b3-3bd514b76051'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to make your prompt public, then you need to make sure that your `handle` is not already in use. You can also continue to improve your prompts and keep saving them which will be recorded as a new commit."
      ],
      "metadata": {
        "id": "zzJLKNCIdv9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loaders - connecting to almost (any) data source you have\n",
        "\n",
        "One of the main challenges in working with LLMs is that they are restricted to the data that they have access to and were trained on. While this is massive, it typically does not include data that is private to you or your organization.\n",
        "\n",
        "For example: as a financial analyst, you might typically work on SEC filings and quarterly reports published by companies that the LLM does not have access to. Langchain provides abstractions that allow us to readily connect to different types of datasources. In the process, it also wraps the input chunking process and adds metadata so that we are able load, chunk and prepare the documents for subsequent calls to the LLM."
      ],
      "metadata": {
        "id": "bZhltFJXN32m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1J4ninDNzKz",
        "outputId": "d3b1ebd9-c997-4fd2-cfa6-7d13b3f7cbd0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.9 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.9 pymupdf-1.24.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(\"/content/VECTOR DATABASES.pdf\")\n",
        "data = loader.load()\n",
        "print (\"The PDF document contains \", len(data), \" pages\")"
      ],
      "metadata": {
        "id": "AG6zRQU8N6FS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ee98c5-ef45-41a1-9a9b-fc67c94d6b9b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The PDF document contains  14  pages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[1:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZZigG7qODlN",
        "outputId": "4ec40e52-f81a-488c-b783-0cf84320d9af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/VECTOR DATABASES.pdf', 'file_path': '/content/VECTOR DATABASES.pdf', 'page': 1, 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Manu: A Cloud Native Vector Database Management System', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with acmart 2020/09/13 v1.73 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creationDate': 'D:20220629005724Z', 'modDate': 'D:20220629005724Z', 'trapped': ''}, page_content='others require some level of guaranteed consistency, i.e., newly\\ninserted data should be visible to queries either immediately or\\nwithin a pre-configured time. Traditional relational databases\\ngenerally support either strong consistency or eventual consis-\\ntency; there is little to no room for customization between these\\ntwo extremes. As such, tunable consistency is a crucial attribute\\nfor cloud-native vector databases.\\n• High hardware cost calls for fine-grained elasticity. Some\\nvector database operations (e.g., vector search and index build-\\ning) are computationally intensive, and hardware accelerators\\n(e.g. GPUs or FPGAs) and/or a large working memory are re-\\nquired for good performance. However, depending on application\\ntypes, workload differs amongst database functionalities. Thus,\\nresources can be wasted or improperly allocated if the vector\\ndatabase does not have fine-grained elasticity. This necessitates\\na careful decoupling of functional and hardware layers; system-\\nlevel decoupling such as separation of read and write logic is\\ninsufficient, elasticity and resource isolation should be managed\\nat the functionalities level rather than the system level.\\nIn summary, modern vector databases should have tunable con-\\nsistency, functionality-level decoupling, and per-component scal-\\nability. Following the design principles of traditional relational\\ndatabases makes achieving these design goals extremely difficult, if\\nnot impossible. A key opportunity for achieving these design goals\\nlies in the potential for relaxing transaction complexity.\\nManu follows the “log as data” paradigm. Specifically, Manu struc-\\ntures the entire system as a group of log publish/subscribe micro-\\nservices. The write-ahead log (WAL) and inter-component mes-\\nsages are published as “logs\", i.e., durable data streams that can be\\nsubscribed. Read-side components, such as search and analytical\\nengines, are all built as log subscribers. This architecture provides\\na simple yet effective way to decouple system functionalities; it\\nenables the decoupling of read from write, stateless from stateful,\\nand storage from computing. Each log entry is assigned a global\\nunique timestamp, and special log entries called time-tick (simi-\\nlar to watermarks in Apache Flink [26]) are periodically inserted\\ninto each log channel signaling the progress of event-time for log\\nsubscribers. The timestamp and time-tick form the basis of the\\ntunable consistency mechanism and multi-version consistency con-\\ntrol (MVCC). To control the consistency level, a user can specify\\na tolerable time lag between a query’s timestamp and the latest\\ntime-tick consumed by a subscriber.\\nAdditionally, we extensively optimize Manu for performance\\nand usability. Manu supports various indexes for vector search, in-\\ncluding vector quantization [22, 34, 37, 83], inverted index [24], and\\nproximity graphs [33]. In particular, we tailor the implementations\\nto better utilize the parallelization capabilities of modern CPUs\\nand GPUs along with the improved read/write speeds of SSDs over\\nHDDs. Manu also integrates refactored functionalities from Mil-\\nvus [81], such as attribute filtering and multi-vector search. More-\\nover, build a visualization tool that allows users to track the perfor-\\nmance of Manu in real time and include an auto-configuration tool\\nthat recommends indexing algorithm parameters using machine\\nlearning.\\nTo summarize, this paper makes the following contributions:\\n• We summarize lessons learned from communicating with over\\n1200 industry users over three years. We shed light on typical\\napplication requirements of vector databases and show how they\\ndiffer from those of traditional relational databases. We then\\noutline the key design goals that vector databases should meet.\\n• We introduce Manu’s key architectural designs as a cloud native\\nvector database, building around the core design philosophy of\\nrelaxing transaction complexity in exchange for tunable consis-\\ntency and fine-grained elasticity.\\n• We present important usability and performance-related en-\\nhancements, e.g., high-level API, a GUI tool, automatic parameter\\nconfiguration, and SSD support.\\nThe rest of the paper is organized as follows. Section 2 pro-\\nvides background on the requirements and design goals of vector\\ndatabases. Section 3 dives deep into Manu’s design. Section 4 high-\\nlights the key features for usability and performance. Section 5\\ndiscusses representative use cases for Manu. Section 6 review re-\\nlated works. Section 7 concludes the paper and outlines future\\nwork.\\n2\\nBACKGROUND AND MOTIVATION\\nConsider video recommendation as a typical use case of vector\\ndatabases. The goal is to help users discover new videos based on\\ntheir personal preferences and previous browsing history. Using\\nmachine learning models (especially deep neural networks), fea-\\ntures of users and videos, such as search history, watch history,\\nage, gender, video language, and tags are converted to embedding\\nvectors. These models are carefully designed and trained to encode\\nthe similarity between user and video vectors into a common vec-\\ntor space. Recommendation is conducted by retrieving candidate\\nvideos from the collection of video vectors via similarity scores\\nwith respect to the specified user vector. The system also needs\\nto handle updates to vectors when new videos are updated, some\\nvideos are deleted and the embedding model is changed.\\nVideo recommendation and other applications of vector databases\\ncan involve hundreds of billions of vectors with daily growth at\\nhundred-million scale, and serve million-scale queries per second\\n(QPS). Existing DBMSs (e.g., relational databases [9, 12], NoSQL [76,\\n86], NewSQL [40, 74]) were not built to manage vector data on that\\nscale. Moreover, the underlying data management requirements of\\ntheir applications differ greatly from vector database applications.\\nFirst, when compared with relational databases, both the archi-\\ntecture and theory of vector databases are far from mature. A key\\nreason for this is that AI- and data-driven applications are still\\nin a state of constant evolution, thereby necessitating continued\\narchitectural and functionality changes to vector databases as well.\\nSecond, complex transactions are unnecessary for vector databases.\\nIn the above example, the recommendation system encodes all se-\\nmantic features of users and videos into standalone vectors as\\nopposed to multi-row or multi-column entity fields in a relational\\ndatabase. As a result, row-level ACID is sufficient; multi-table oper-\\nations (such as joins) are inessential.\\nThird, vector database applications need a flexible performance-\\nconsistency trade-off. While some applications adopt a strong or\\neventual consistency model, there are others that fall between the\\ntwo extremes. Users may wish to relax consistency constraints in\\n'),\n",
              " Document(metadata={'source': '/content/VECTOR DATABASES.pdf', 'file_path': '/content/VECTOR DATABASES.pdf', 'page': 2, 'total_pages': 14, 'format': 'PDF 1.5', 'title': 'Manu: A Cloud Native Vector Database Management System', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with acmart 2020/09/13 v1.73 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'creationDate': 'D:20220629005724Z', 'modDate': 'D:20220629005724Z', 'trapped': ''}, page_content='exchange for better system throughput. In the video recommen-\\ndation example, observing a newly uploaded video after several\\nseconds is acceptable but keeping users waiting for recommenda-\\ntion harms user experience. Thus, the application can configure the\\nallowed maximal delay for the video updates in order to improve\\nsystem throughput.\\nFourth, vector databases have more stringent and diversified\\nhardware requirements compared with traditional databases. This\\nis attributed to three reasons. First, vector database operations\\nare computation-intensive, and thus hardware accelerators such\\nas GPUs are critical for computing functionalities such as search\\nand indexing. Second, accesses to vector data (e.g., search or up-\\ndate) generally have poor locality, thereby requiring large RAM\\nfor good performance. Third, different applications vary signifi-\\ncantly in their resource demands for the system functionalities.\\nCore functionalities of a vector database include data insertion,\\nindexing, filtering, and vector search. Applications such as video\\nrecommendation require online insertion and high concurrency\\nvector search. In contrast, for interactive use cases such as drug dis-\\ncovery, offline data ingestion and indexing are generally acceptable.\\nAlthough interactive applications usually require lower throughput\\nthan recommendation systems, they have high demands for real-\\ntime filtering, similarity-based vector search, and hybrid queries.\\nThe high hardware costs as well as diverse workload features call\\nfor fine-grained elasticity.\\nThe key design goals of Manu are summarized below; these\\ndesign goals not only fully encompass the above characteristics but\\nalso share some common goals with generic cloud-based databases.\\n• Long-term evolvability: Overall system complexity must be\\ncontrolled for the continuous evolution of Manu’s functionalities.\\nWithout the need to support complex transactions, there lies an\\nopportunity to model all the event sequences (such as WAL and\\ninter-component messages) as message queues to cleanly decou-\\nple the entire system. In this way, individual components can\\nevolve, be added, or be replaced easily with minimal interference\\nto other components. This design echos large-scale data analytic\\nplatforms, which often rely on data streaming systems such as\\nKafka to connect system components.\\n• Tunable consistency: To enable flexible consistency-performance\\ntrade-off, Manu should introduce delta consistency that falls be-\\ntween strong consistency and eventual consistency, where a read\\noperation returns the last value that was produced at most delta\\ntime units preceding itself. It’s worth noting that strong consis-\\ntency and eventual consistency can be realized as special cases\\nof this model, with delta being zero and infinity, respectively.\\n• Good elasticity: Workload fluctuations can cause different loads\\non individual system components. In order to dynamically al-\\nlocate compute resources to high-load tasks, components must\\nbe carefully decoupled, taking both functionality and hardware\\ndependencies into consideration. System elasticity and resource\\nisolation should be managed at the component-level rather than\\nat the system-level (e.g. decoupling indexing from querying ver-\\nsus decoupling read from write).\\n• High availability: Availability is a must-have for modern cloud-\\nbased applications; Manu must isolate system failures at the\\ncomponent level and make failure recovery transparent.\\nPrimary Key Feature Vector Label\\nNumerical attribute\\nLSN\\nUser Fields\\nSystem Fields\\nID\\nEmbedding\\nFiltering Field\\nFigure 1: An example of Manu’s schema.\\n• High performance: Query processing performance is key to\\nvector databases. For good performance, implementations to be\\nextensively optimized for hardware. Moreover, the framework\\nshould be carefully designed so as to minimize system overheads\\nfor query serving.\\n• Strong adaptability: Our customers use vector databases in a\\nvariety of environments, ranging from prototyping on laptops to\\nlarge-scale deployments on the cloud. A vector database should\\nprovide consistent user experience and reduce code/data migra-\\ntion overhead across environments.\\n3\\nTHE MANU SYSTEM\\nIn this section, we begin by first introducing the basic concepts of\\nManu. Next, we present the system designs, including the overall\\nsystem architecture, the log backbone, and how Manu conducts\\nvector searches and builds vector search indexes.\\n3.1\\nSchema, Collection, Shard, and Segment\\nSchema: The basic data types of Manu are vector, string, boolean,\\ninteger, and floating point. A schema example is given in Figure 1.\\nSuppose each entity consists of five fields and corresponds to a\\nproduct on an e-commerce platform. The Primary key is the ID\\nof the entity. It can either be an integer or a string. If users do\\nnot specify this field, the system will automatically add an integer\\nprimary key for each entity. The Feature vector is the embedding of\\nthe product. The Label is the category of the product, such as food,\\nbook, and cloth. The Numerical attribute is a float or an integer\\nassociated with the product, such as price, weight, or production\\ndate. Manu supports multiple labels and numerical attributes in\\neach entity. Note that these fields are used for filtering, rather than\\njoin or aggregation. The Logical sequence number (LSN) is a system\\nfield hidden from users.\\nCollection: A Collection is a set of entities similar to the concept of\\ntables in relational databases. For example, a collection can contain\\nall the products of an e-commerce platform. The key difference is\\nthat collections have no relations with each other; thus, relational\\nalgebra, such as join operations, are not supported.\\nShard: The Shard correspondence to insertion/deletion channel.\\nEntities are hashed into multiple shards based on their primary keys\\nduring insertion/deletion. Manu’s data placement unit is segment\\nrather than shard. 1\\nSegment: Entities from each shard are organized into segments. A\\nsegment can be in either a growing or sealed state. Sealed segments\\nare read-only while growing segments can accept new entities. A\\ngrowing segment will switch to sealed state when it reaches a prede-\\nfined size (set to 512MB by default) or if a period of time has passed\\n1Using segments for data placement is more flexible than shards, as the number of\\nshards is static, while the number of segments grows as the volume of the collection\\nincreases.\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDFs are not the only type of document loaders that are available. There are several interesting implementations like the `ConfluenceLoader` that allows you to load documents from your organizations confluence space and possibly build a Question-Answer bot that can be used by new joiners!\n",
        "\n",
        "```python\n",
        "from langchain.document_loaders import ConfluenceLoader\n",
        "\n",
        "loader = ConfluenceLoader(\n",
        "    url=\"https://orgname.atlassian.net/wiki\",\n",
        "    username=\"org_username\",\n",
        "    api_key=\"ORG_API_KEY\"\n",
        ")\n",
        "\n",
        "documents = loader.load(space_key=\"SPACE\", include_attachments=True, limit=2, max_pages=10)\n",
        "```"
      ],
      "metadata": {
        "id": "4FMvIfmbOFvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Managing the context window\n",
        "\n",
        "One of the next challenges in working with LLMs is about what parts of a document to present in the prompt as context. Let's consider the PDF above and let's assume that we want to build a Retrieval Augmented Generation application that helps to answer users questions based on the paper. If a user asks the question: \"What is DCLM\", then we must find the right sentences or pragraphs from this document that can be added to the prompt so that the LLM can then answer the question.\n",
        "\n",
        "In order to find the right set of sentences, we must first be able to split up the document into chunks and then perform subsequent steps like embeddings, or making multiple calls to the LLM. We will see this in action during the project but the below cell shows you how Langchain provides an easy for us to manage this."
      ],
      "metadata": {
        "id": "MBGaXPENNqu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 2000,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        ")"
      ],
      "metadata": {
        "id": "fSqzeg9DNtEp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = text_splitter.create_documents([data[0].page_content])"
      ],
      "metadata": {
        "id": "HJ2JJbfbnkzc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"You now have {len(docs)} docs intead of 1 piece of text\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4_kZw-INvGI",
        "outputId": "e1a5cc81-2f3a-4760-b5be-598dccce289a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You now have 3 docs intead of 1 piece of text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqwPDnyaNwvh",
        "outputId": "11b87dd5-f562-406b-947d-4db4c2433b8f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Manu: A Cloud Native Vector Database Management System\\nRentong Guo†∗, Xiaofan Luan†∗, Long Xiang‡∗, Xiao Yan‡∗, Xiaomeng Yi†∗, Jigao Luo†§\\nQianya Cheng†, Weizhi Xu†, Jiarui Luo‡, Frank Liu†, Zhenshan Cao†, Yanliang Qiao†, Ting Wang†\\nBo Tang‡ Charles Xie†\\n†Zilliz\\n‡Department of Computer Science and Engineering, Southern University of Science and Technology\\n§Technical University of Munich\\n†{firstname.lastname}@zilliz.com\\n‡{xiangl3@mail., yanx@, 11911419@mail., tangb3@}sustech.edu.cn, §jigao.luo@tum.de\\nABSTRACT\\nWith the development of learning-based embedding models, embed-\\nding vectors are widely used for analyzing and searching unstruc-\\ntured data. As vector collections exceed billion-scale, fully managed\\nand horizontally scalable vector databases are necessary. In the\\npast three years, through interaction with our 1200+ industry users,\\nwe have sketched a vision for the features that next-generation\\nvector databases should have, which include long-term evolvability,\\ntunable consistency, good elasticity, and high performance.\\nWe present Manu, a cloud native vector database that imple-\\nments these features. It is difficult to integrate all these features\\nif we follow traditional DBMS design rules. As most vector data\\napplications do not require complex data models and strong data\\nconsistency, our design philosophy is to relax the data model and\\nconsistency constraints in exchange for the aforementioned fea-\\ntures. Specifically, Manu firstly exposes the write-ahead log (WAL)\\nand binlog as backbone services. Secondly, write components are\\ndesigned as log publishers while all read-only analytic and search\\ncomponents are designed as independent subscribers to the log ser-\\nvices. Finally, we utilize multi-version concurrency control (MVCC)\\nand a delta consistency model to simplify the communication and\\ncooperation among the system components. These designs achieve\\na low coupling among the system components, which is essential'),\n",
              " Document(page_content='for elasticity and evolution. We also extensively optimize Manu for\\nperformance and usability with hardware-aware implementations\\nand support for complex search semantics. Manu has been used\\nfor many applications, including, but not limited to, recommenda-\\ntion, multimedia, language, medicine and security. We evaluated\\nManu in three typical application scenarios to demonstrate its effi-\\nciency, elasticity, and scalability.\\nPVLDB Reference Format:\\nRentong Guo, Xiaofan Luan, Long Xiang, Xiao Yan, Xiaomeng Yi, Jigao Luo,\\nQianya Cheng, Weizhi Xu, Jiarui Luo, Frank Liu, Zhenshan Cao, Yanliang\\nQiao, Ting Wang, Bo Tang, and Charles Xie. Manu: A Cloud Native Vector\\nDatabase Management System. PVLDB, 15(12): XXX-XXX, 2022.\\ndoi:XX.XX/XXX.XX\\n∗Co-first-authors are ordered alphabetically.\\n‡ Work done while working with Zilliz, correspondence to Bo Tang.\\nThis work is licensed under the Creative Commons BY-NC-ND 4.0 International\\nLicense. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of\\nthis license. For any use beyond those covered by this license, obtain permission by\\nemailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights\\nlicensed to the VLDB Endowment.\\nProceedings of the VLDB Endowment, Vol. 15, No. 12 ISSN 2150-8097.\\nPVLDB Artifact Availability:\\nThe source code, data, and/or other artifacts have been made available at\\nhttps://github.com/milvus-io/milvus/tree/2.0.\\n1\\nINTRODUCTION\\nAccording to IDC, unstructured data, such as text, images, and video,\\ntook up about 80% of the 40,000 exabytes of new data generated in\\n2020, their percentage keeps rising due to the increasing amount\\nof human-generated rich media [48]. With the rise of learning-\\nbased embedding models, especially deep neural networks, using\\nembedding vectors to manage unstructured data has become com-\\nmonplace in many applications such as e-commerce, social media,\\nand drug discovery [49, 63, 68]. A core feature of these applica-')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While `RecursiveCharacterTextSplitter` is the simplest option, there are other implemented classes like\n",
        "\n",
        "- `TiktokenText` - splits based on the number of tokens as calculated by OpenAI\n",
        "- `PythonCodeTextSplitter` - ensures that Python class definitions & functions are kept together\n",
        "- `LatexTextSplitter` - ensures that Latex headings, sub-headings, enumerations etc. are respected when chunking the input"
      ],
      "metadata": {
        "id": "Pg5KKrewN1eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating our first chain - LCEL\n",
        "\n",
        "Till now, we have seen how to manage prompts, read in documents, split them into chunks. Let's move onto the actual LLM integration and make the first call to an LLM. This is where we can introduce the Langchain Expression Language (LCEL).\n",
        "\n",
        "LCEL is nothing but a simple way to creating a chain - or better said chaining together multiple things to achieve a certain operation. For instance, let's say that we want to summarize the above research paper. We already have the prompt, we also read in the data and then we chain it together with the LLM.\n",
        "\n",
        "This is achieved with the help of the Pipe operator - this is a common practice when writing Unix commands and is inspired from there. Let's start by creating our first chain - the summarization chain."
      ],
      "metadata": {
        "id": "hBm77xmdOdhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizationChain = prompt_template | llm"
      ],
      "metadata": {
        "id": "PuqFnnB6bvr9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizationChain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thMhwWCxb2ai",
        "outputId": "35df0704-ea53-4ee1-8629-1fd7aadabe0f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['input_text'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful summarization assistant.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_text'], template='Identify the highlights from the following piece of text and return it as a bullet list\\n                                                while focussing on retaining as much specific details as you can.\\n                                                The returned bullet list must be formatted as \\n- Fact 1 about the information \\n- Fact 2\\n                                                about the information.\\n                                                {input_text}\\n                                              '))])\n",
              "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x79b5b05dd630>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x79b5b05def50>, openai_api_key=SecretStr('**********'), openai_proxy='')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summarization chain is nothing but an abstract representation of our prompt_template chained with an LLM. We can easily invoke this chain by providing any piece of text as input. Let's consider the same \"Club-Mate\" text as we created originally to keep it simple."
      ],
      "metadata": {
        "id": "s9AMOmI6omSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizationChain.invoke({'input_text': text})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZnTXb36b-fQ",
        "outputId": "a4673d6e-d653-4f3b-dbab-6771bcfbd5c1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='- Club-Mate is a caffeinated carbonated mate-extract beverage with 20 mg of caffeine per 100 ml.\\n- It is made by the Loscher Brewery in Münchsteinach, Germany, originating in 1924.\\n- The beverage has a sugar content of 5 g per 100 ml and 20 kcal per 100 ml, lower than most energy drinks.\\n- Club-Mate is available in 0.33-litre and 0.5-litre bottles.\\n- Some bottles of Club-Mate feature the slogan \"man gewöhnt sich daran\", meaning \"you\\'ll get used to it\".\\n- Examples of Club-Mate-based mixed drinks include vodka-mate, Tschunk (rum and Club-Mate), and Jaeger-Mate (Jägermeister and Club-Mate).', response_metadata={'token_usage': {'completion_tokens': 169, 'prompt_tokens': 289, 'total_tokens': 458}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d9a69e35-7dd8-4c76-911d-faa244d72c57-0', usage_metadata={'input_tokens': 289, 'output_tokens': 169, 'total_tokens': 458})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will be able to see from the results that the OpenAI model has been called and the response is shown as the AIMessage. Instead of seeing the actual object, we typically want to work with the string response directly. We can easily obtain this through the help of an output_parser.\n",
        "\n",
        "In this case, we are adding the simplest output_parser that extracts the string value from the response. There are other variants of the output_parse (like JSON, CSV etc.) which are useful based on how you structure your prompt."
      ],
      "metadata": {
        "id": "Nd4KSEN8eQxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "Sk2XN5ZAc4CD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizationChain = prompt_template | llm | output_parser"
      ],
      "metadata": {
        "id": "MhEAP4u6c6vW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizationChain.invoke({'input_text': text})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "u6gucqRrc_EK",
        "outputId": "41ef301f-17ed-47c5-de42-5db54ec80f5b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- Club-Mate is a caffeinated carbonated mate-extract beverage made by the Loscher Brewery in Münchsteinach, Germany since 1924.\\n- It contains 20 mg of caffeine, 5 g of sugar, and 20 kcal per 100 ml, making it lower in caffeine and sugar content compared to most energy drinks.\\n- Club-Mate is available in 0.33-litre and 0.5-litre bottles.\\n- Some bottles feature the slogan \"man gewöhnt sich daran\" which translates to \"you\\'ll get used to it.\"\\n- Mixed drinks using Club-Mate include vodka-mate, Tschunk (rum and Club-Mate), and Jaeger-Mate (Jägermeister and Club-Mate).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw the simplest case above where the input was just a plain text which is passed in directly. Let's move to the case of our PDF document. We have already created the chunks and would like the summarization function to now apply to each of these chunks. We will use the same chain as we defined above but we invole it using the `batch` mode where all the calls are made in parallel and the responses are returned as a list."
      ],
      "metadata": {
        "id": "NDIzUCJLeam1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputList = [{'input_text':x.page_content} for x in docs]"
      ],
      "metadata": {
        "id": "vuT1Ehppqji7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizationChain.batch(inputList)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wkmt2Tbz2UTs",
        "outputId": "0fc6e298-8cdb-4bc3-9238-7fe48597d0a2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['- Manu is a cloud native vector database management system.\\n- It was developed based on the feedback and needs of over 1200 industry users.\\n- Features of the next-generation vector databases include long-term evolvability, tunable consistency, good elasticity, and high performance.\\n- To implement these features, Manu relaxes data model and consistency constraints in exchange for the desired functionalities.\\n- Manu utilizes write-ahead log (WAL) and binlog as backbone services, with write components as log publishers and analytic/search components as independent subscribers.\\n- Multi-version concurrency control (MVCC) and a delta consistency model are employed to simplify communication and cooperation among system components.',\n",
              " '- Manu is a cloud-native vector database management system.\\n- It is optimized for performance and usability with hardware-aware implementations and support for complex search semantics.\\n- Manu has been used for various applications, including recommendation, multimedia, language, medicine, and security.\\n- The efficiency, elasticity, and scalability of Manu were demonstrated in three typical application scenarios.\\n- Unstructured data like text, images, and video accounted for about 80% of the 40,000 exabytes of new data generated in 2020, with the percentage increasing due to human-generated rich media.\\n- The use of embedding vectors to manage unstructured data has become common in applications such as e-commerce, social media, and drug discovery.',\n",
              " '- Embedding vectors encode semantics of unstructured data into high-dimensional vector space for operations like recommendation and search.\\n- Milvus, a vector database, was open sourced in 2019 and later restructured for cloud-native architecture based on feedback from industry users.\\n- Manu, the restructured database, focuses on a single vector representation for complex data, row-level ACID transactions, and tunable performance-consistency trade-offs.']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A simple LCEL chain\n",
        "\n",
        "In the previous section, we combined the prompt tempalte with an LLM to create a chain. We could then invoke this chain with single pieces of text or also run it in a batch mode. Now we will also introduce a last component called as the `output_parser`.\n",
        "\n",
        "If you notice in the answers above, they are in the form of an AIMessage object which contains several pieces of information like metadata, number of tokens etc. However, we are only interested in the final summary of this paper. We can introduce a simple output_parser into this chain that extracts the relevant aspects for us. It is as simple as piping in another element at the end of the chain like so:"
      ],
      "metadata": {
        "id": "u2GdkFGjva9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "summarizationChain = prompt_template | llm | output_parser"
      ],
      "metadata": {
        "id": "S-2fpIBskmtW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What will happen now is that instead of presenting to us the raw AIMessage object, the output_parser will only extract the content from the object. This is just one example of the output parser but there are others including custom ones that you could define based on how you would like to process the output - e.g. JSON based field extraction."
      ],
      "metadata": {
        "id": "ayetIZk7k4kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizationChain.invoke({'input_text': text})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "cq10Hklhk2iQ",
        "outputId": "26fdd65a-ffc2-43e6-a808-6b93fc33f50f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- Club-Mate is a caffeinated carbonated mate-extract beverage made by the Loscher Brewery in Münchsteinach, Germany since 1924.\\n- It contains 20 mg of caffeine, 5 g of sugar, and has 20 kcal per 100 ml, making it lower in sugar content compared to most energy drinks.\\n- Club-Mate is available in 0.33-litre and 0.5-litre bottles.\\n- Some bottles of Club-Mate feature the slogan \"man gewöhnt sich daran,\" translating to \"you\\'ll get used to it.\"\\n- Club-Mate can be used in mixed drinks like vodka-mate, Tschunk (rum and Club-Mate), and Jaeger-Mate (Jägermeister and Club-Mate).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extending our Chain"
      ],
      "metadata": {
        "id": "3yaXL2xJDc0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, to show why the use of Langchain is relevant - let's say that instead of using OpenAI with this application we want to use a different LLM provider - maybe Anthropic. We can easily do this by swapping out the definition of the LLM with the new one while the entire rest of our code remains the same. This is also typically used to provide a failover functionality when one LLM API goes down or takes too long to respond."
      ],
      "metadata": {
        "id": "sL7dqtvntxs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-wOj_9TCq94",
        "outputId": "630243b1-1cbb-45d6-ffc0-7a3ce688f665"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-anthropic\n",
            "  Downloading langchain_anthropic-0.1.22-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting anthropic<1,>=0.28.0 (from langchain-anthropic)\n",
            "  Downloading anthropic-0.32.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from langchain-anthropic) (0.7.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.26 in /usr/local/lib/python3.10/dist-packages (from langchain-anthropic) (0.2.27)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain-anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic<1,>=0.28.0->langchain-anthropic) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain-anthropic) (0.27.0)\n",
            "Collecting jiter<1,>=0.4.0 (from anthropic<1,>=0.28.0->langchain-anthropic)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain-anthropic) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain-anthropic) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain-anthropic) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.28.0->langchain-anthropic) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-anthropic) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-anthropic) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-anthropic) (0.1.96)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-anthropic) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-anthropic) (8.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.28.0->langchain-anthropic) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic<1,>=0.28.0->langchain-anthropic) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.28.0->langchain-anthropic) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic<1,>=0.28.0->langchain-anthropic) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<1,>=0.28.0->langchain-anthropic) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain-anthropic) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-anthropic) (3.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-anthropic) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<1,>=0.28.0->langchain-anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic<1,>=0.28.0->langchain-anthropic) (2.20.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic<1,>=0.28.0->langchain-anthropic) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.28.0->langchain-anthropic) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.28.0->langchain-anthropic) (2024.6.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<1,>=0.28.0->langchain-anthropic) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-anthropic) (2.0.7)\n",
            "Downloading langchain_anthropic-0.1.22-py3-none-any.whl (20 kB)\n",
            "Downloading anthropic-0.32.0-py3-none-any.whl (866 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.6/866.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, anthropic, langchain-anthropic\n",
            "Successfully installed anthropic-0.32.0 jiter-0.5.0 langchain-anthropic-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "anthropic_api_key = userdata.get('CLAUDE_API_KEY')\n",
        "claude_llm = ChatAnthropic(model='claude-3-opus-20240229', anthropic_api_key=anthropic_api_key)"
      ],
      "metadata": {
        "id": "fOdoc3NjCuUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claude_summarizationChain = prompt_template | claude_llm | output_parser\n",
        "claude_summarizationChain.invoke({'input_text': text})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "OYVUT0SaDanZ",
        "outputId": "2f4dee56-b5ee-4883-c3a9-a20d49d4eb40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- Club-Mate is a caffeinated carbonated mate-extract beverage made by the Loscher Brewery in Münchsteinach, Germany, which originated in 1924\\n- Club-Mate has 20 mg of caffeine per 100 ml, sugar content of 5 g per 100 ml, and 20 kcal per 100 ml, which is lower than most energy drinks\\n- Club-Mate is available in 0.33-litre and 0.5-litre bottles\\n- Some Club-Mate bottles include the slogan \"man gewöhnt sich daran\", which roughly translates as \"you\\'ll get used to it\"\\n- Examples of Club-Mate-based mixed drinks are: vodka-mate; Tschunk, a combination of rum and Club-Mate; Jaeger-Mate, a mix of Jägermeister and Club-Mate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}